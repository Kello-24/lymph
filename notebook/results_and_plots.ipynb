{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A hidden Markov model for lymphatic tumour progression in head and neck cancer\n",
    "\n",
    "Roman Ludwig¹*, Bertrand Pouymayou¹, Panagiotis Balermpas¹ and Jan Unkelbach¹\n",
    "\n",
    "¹ Departement of Radiation Oncology, University Hospital Zurich, Switzerland \\\n",
    "\\* [roman.ludwig@usz.ch](mailto:roman.ludwig@usz.ch)\n",
    "\n",
    "***\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Currently , elective clinical target volume (CTV-N) definition for head & neck squamous cell carcinoma (HNSCC) is mostly based on the prevalence of nodal involvement for a given tumor location. In this work, we propose a probabilistic model for lymphatic metastatic spread that can quantify the risk of microscopic involvement in lymph node levels (LNL) given the location of macroscopic metastases and T-stage. This may allow for further personalized CTV-N definition based on an individual patient’s state of disease. \\\n",
    "We model the patient's state of metastatic lymphatic progression as a collection of hidden binary random variables that indicate the involvement of LNLs. In addition, each LNL is associated with observed binary random variables that indicate whether macroscopic metastases are detected. A hidden Markov model (HMM) is used to compute the probabilities of transitions between states over time. The underlying graph of the HMM represents the anatomy of the lymphatic drainage system. Learning of the transition probabilities is done via Markov chain Monte Carlo sampling and is based on a dataset of HNSCC patients in whom involvement of individual LNLs was report-ed. \\\n",
    "The model is demonstrated for ipsilateral metastatic spread in oropharyngeal HNSCC patients. We demonstrate the model's capability to quantify the risk of microscopic involvement in levels III and IV, depending on whether macroscopic metastases are observed in the upstream levels II and III, and depending on T-stage. \\\n",
    "In conclusion, the statistical model of lymphatic progression may inform future, more personal-ized, guidelines on which LNL to include in the elective CTV. However, larger multi-institutional datasets for model parameter learning are required for that. \n",
    "\n",
    "***\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook contains all the code we ran to produce results and plots for our paper. It is intended to be read alongside the paper if one wants to understand better how exactly we implemented and used the methodology introduced in the paper. However, this notebook is NOT a stand-alone work but only a supplement.\n",
    "\n",
    "### Imports\n",
    "\n",
    "First, we import some libraries that are necessary for our implementation. [`lymph`](https://github.com/rmnldwg/lymph) is the package we wrote, while [`corner`](https://corner.readthedocs.io/en/latest/) and [`emcee`](https://emcee.readthedocs.io/en/stable/) (see also the corresponding [arXiv paper](https://arxiv.org/abs/1202.3665) on this package) are both packages by Dan Foreman-Mackey & contributors. [`matplotlib`](https://matplotlib.org/stable/index.html) is an extensive and powerful plotting library. All other packages are standard and included with any default python installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic stuff\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import datetime as dt\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gs\n",
    "import matplotlib.font_manager as font_manager\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import ListedColormap\n",
    "from cycler import cycler\n",
    "import corner\n",
    "\n",
    "# sampling\n",
    "import emcee\n",
    "import tqdm\n",
    "\n",
    "# our package\n",
    "import lymph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "The variables below are meant to be constants. `T_MAX` is the length of the used binomial time-prior. `DRAW_SAMPLES` is a `bool` that defines whether new samples will be drawn for the plots and computations or alreadz drawn samples should be loaded from file. `SEED` is the seed for the random number generator in `numpy` and aims at reproducability. `SAVE_FIGURES` defines whether or not generated figures should be saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_MAX = 10\n",
    "DRAW_SAMPLES = True\n",
    "SEED = 42\n",
    "SAVE_FIGURES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colors\n",
    "\n",
    "We chose the colors of the University Hospital Zurich's corporate design for the default colors of our plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USZ colors\n",
    "usz_blue = '#005ea8'\n",
    "usz_green = '#00afa5'\n",
    "usz_red = '#ae0060'\n",
    "usz_orange = '#f17900'\n",
    "usz_gray = '#c5d5db'\n",
    "\n",
    "# colormaps\n",
    "white_to_blue  = LinearSegmentedColormap.from_list(\"white_to_blue\", \n",
    "                                                   [\"#ffffff\", usz_blue], \n",
    "                                                   N=256)\n",
    "white_to_green = LinearSegmentedColormap.from_list(\"white_to_green\", \n",
    "                                                   [\"#ffffff\", usz_green], \n",
    "                                                   N=256)\n",
    "green_to_red   = LinearSegmentedColormap.from_list(\"green_to_red\", \n",
    "                                                   [usz_green, usz_red], \n",
    "                                                   N=256)\n",
    "\n",
    "h = usz_gray.lstrip('#')\n",
    "gray_rgba = tuple(int(h[i:i+2], 16) / 255. for i in (0, 2, 4)) + (1.0,)\n",
    "tmp = LinearSegmentedColormap.from_list(\"tmp\", [usz_green, usz_red], N=128)\n",
    "tmp = tmp(np.linspace(0., 1., 128))\n",
    "tmp = np.vstack([np.array([gray_rgba]*128), tmp])\n",
    "halfGray_halfGreenToRed = ListedColormap(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Settings\n",
    "\n",
    "Here we define a function to consistently set the size of our plots and we load the default settings regarding font size etc from an `.mplstyle` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\".mplstyle\")\n",
    "\n",
    "def set_size(width=\"single\", unit=\"cm\", ratio=\"golden\"):\n",
    "    if width == \"single\":\n",
    "        width = 10\n",
    "    elif width == \"full\":\n",
    "        width = 16\n",
    "    else:\n",
    "        try:\n",
    "            width = width\n",
    "        except:\n",
    "            width = 10\n",
    "            \n",
    "    if unit == \"cm\":\n",
    "        width = width / 2.54\n",
    "        \n",
    "    if ratio == \"golden\":\n",
    "        ratio = 1.618\n",
    "    else:\n",
    "        ratio = ratio\n",
    "    \n",
    "    try:\n",
    "        height = width / ratio\n",
    "    except:\n",
    "        height = width / 1.618\n",
    "        \n",
    "    return (width, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The dataset we use here was reconstructed by [Pouymayou et al.](#pouymayou) from [Sanguineti et al.](#sanguineti). Here, it is loaded from file. Note that $N_0$ patients were added to make up 30% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and risk estimation from Pouymayou et al [2]\n",
    "pouymayou_params = [0.061, 0.638, 0.094, 0.057, 0.08, 0.331, 0.242]\n",
    "pouymayou_MLrisk = np.array([[ 1.53,  1.64,  1.66,  1.56], \n",
    "                             [24.67, 81.55, 89.64, 39.06], \n",
    "                             [ 4.48,  9.97, 59.93, 38.75], \n",
    "                             [ 1.83,  2.25,  6.05,  4.44]]) / 100.\n",
    "\n",
    "# data reconstructed from Sanguineti et al [1] (without N0 patients)\n",
    "data = pd.read_csv(\"./data/2009_sanguineti.csv\", \n",
    "                   header=None, \n",
    "                   names=['I', 'II', 'III', 'IV'])\n",
    "\n",
    "# inserting info about the \"T-stage\"\n",
    "data.insert(0, \"T-stage\", [\"early\"] * data.shape[0])\n",
    "\n",
    "columns = pd.MultiIndex.from_arrays([['Info', 'path', 'path', 'path', 'path'], \n",
    "                                     ['T-stage', 'I', 'II', 'III', 'IV']])\n",
    "data = pd.DataFrame(data.values.tolist(), columns=columns)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "In this section we will set up everything necessary to perform inference on the dataset recostructed by [[2]](#pouymayou) from [[1]](#sanguineti).\n",
    "\n",
    "### Lymphatic Network\n",
    "\n",
    "Here, we need to define the underlying anatomical network of lymph node levels (LNLs), as it is also defined in [[2]](#pouymayou). \n",
    "\n",
    "The tumor and every LNL are represented by a key in a dictionary called `graph` each. The respective value in the dictionary is a list of nodes it drains to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract representation of the lymphatic network\n",
    "graph = {'T'  : ['I', 'II', 'III', 'IV'], \n",
    "         'I'  : ['II'], \n",
    "         'II' : ['III'], \n",
    "         'III': ['IV'], \n",
    "         'IV' : []}\n",
    "\n",
    "systm = lymph.System(graph=graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Prior\n",
    "We need to choose a time prior for the parameter learning. A [Binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution) was chosen for its intuitively meaningful shape and simple structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=set_size());\n",
    "\n",
    "for i,p in enumerate([0.4, 0.55, 0.7]):\n",
    "    ax.plot(np.arange(T_MAX+1), \n",
    "            sp.stats.binom.pmf(np.arange(T_MAX+1), T_MAX, p), \n",
    "            \"o-\", label=f\"$p = {{{p}}}$\")\n",
    "    \n",
    "ax.set_xlim([0,10]);\n",
    "ax.set_ylim([-0.02,0.35])\n",
    "ax.set_xlabel(\"time step $t$\");\n",
    "ax.set_ylabel(r\"$p(t)$\");\n",
    "ax.tick_params();\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary of time priors for the sampling process\n",
    "time_prior_dict = {}\n",
    "time_prior_dict[\"early\"] = sp.stats.binom.pmf(np.arange(T_MAX+1), T_MAX, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning (HMM)\n",
    "\n",
    "Using the anatomical model along with the data, we can now load the data into the lymph system class. To do so we must also pass a dictionary called `spsn_dict`. **``spsn``** stands for **sp**ecificity and **s**e**n**sitivity. In that dictionary, one has to define a list ``[specificty, sensitivity]`` for each diagnostic modality one is interested. So if we, for example, have MRI and CT data, then we would pass a dictionary like this:\n",
    "\n",
    "```python\n",
    "spsn_dict = {\"MRI\": [spec_MRI, sens_MRI], \n",
    "             \"CT\" : [spec_CT , sens_CT ]}\n",
    "```\n",
    "\n",
    "Note however, that the keys of this dictionary must be diagnostic modalities that are also present in the dataset. More precisely, they must be the overarching categories in the ``MultiIndex`` under which one then finds the individual LNLs.\n",
    "\n",
    "Finally, we can use the likelihood function that is built into the `lymph` package together with the sampling implementation `emcee` to infer the base probabilities $b_{v}$ and transition probabilities $t_{\\text{pa}(v)v}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define specificity and sensitivity for diagnostic modalities\n",
    "spsn_dict = {\"path\": [1., 1.]}\n",
    "\n",
    "# load data\n",
    "systm.load_data(data, t_stage=[\"early\"], spsn_dict=spsn_dict, mode=\"HMM\")\n",
    "\n",
    "# check if likelihood works\n",
    "assert systm.likelihood(np.random.uniform(size=(7,)), \n",
    "                        t_stage=[\"early\"], \n",
    "                        time_prior_dict=time_prior_dict, \n",
    "                        mode=\"HMM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the sampler\n",
    "ndim, nwalker, nstep, burnin = 7, 200, 2000, 1000\n",
    "moves = [(emcee.moves.DEMove(), 0.8), (emcee.moves.DESnookerMove(), 0.2)]\n",
    "\n",
    "if DRAW_SAMPLES:\n",
    "    # starting point\n",
    "    np.random.seed(SEED)\n",
    "    theta0 = np.random.uniform(low=0., high=1., size=(nwalker,ndim))\n",
    "\n",
    "    # the actual sampling round\n",
    "    if __name__ == \"__main__\":\n",
    "        with Pool() as pool:\n",
    "            sampler = emcee.EnsembleSampler(nwalker, ndim, systm.likelihood, \n",
    "                                            args=[['early'], time_prior_dict], \n",
    "                                            moves=moves, pool=pool)\n",
    "            sampler.run_mcmc(theta0, nstep, progress=True)\n",
    "\n",
    "    # extracting 200,000 of the 400,000 samples\n",
    "    samples_HMM = sampler.get_chain(flat=True, discard=burnin)\n",
    "\n",
    "    # saving the sampled data to disk for later convenience\n",
    "    np.save(\"./samples/HMM.npy\", samples_HMM)\n",
    "    \n",
    "else:\n",
    "    # loading in case we don't want to draw all the samples again\n",
    "    samples_HMM = np.load(\"./samples/HMM.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DRAW_SAMPLES:\n",
    "    # check acceptance faction of the sampler to get an indication on whether sth\n",
    "    # went wrong or not\n",
    "    ar = np.mean(sampler.acceptance_fraction)\n",
    "    print(f\"the HMM sampler accepted {ar * 100 :.2f} % of samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [r\"$\\tilde{b}_1$\", r\"$\\tilde{b}_2$\", \n",
    "          r\"$\\tilde{b}_3$\", r\"$\\tilde{b}_4$\", \n",
    "          r\"$\\tilde{t}_{12}$\", r\"$\\tilde{t}_{23}$\", r\"$\\tilde{t}_{34}$\"]\n",
    "\n",
    "fig = plt.figure(figsize=set_size(width=\"full\", ratio=1));\n",
    "\n",
    "# using the corner plot package\n",
    "corner.corner(\n",
    "    samples_HMM,\n",
    "    labels=labels,\n",
    "    smooth=True,\n",
    "    fig=fig,\n",
    "    color=usz_blue,\n",
    "    hist_kwargs={'histtype': 'stepfilled', 'color': usz_blue},\n",
    "    **{\n",
    "        \"plot_datapoints\": False,\n",
    "        \"plot_density\": True,\n",
    "        \"no_fill_contours\": True,\n",
    "        \"contour_kwargs\": {\"colors\": \"black\"},\n",
    "        \"levels\": np.array([0.2, 0.5, 0.8])\n",
    "    },\n",
    "    show_titles=True,\n",
    "    title_kwargs={\"fontsize\": \"medium\"}\n",
    ");\n",
    "\n",
    "axes = fig.get_axes();\n",
    "for ax in axes:\n",
    "    ax.grid(False);\n",
    "    \n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/corner_HMM.png\", dpi=300, bbox_inches=\"tight\");\n",
    "    plt.savefig(\"./figures/corner_HMM.svg\", bbox_inches=\"tight\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Matrix\n",
    "\n",
    "We can now set the hidden Markov model's parameters to the expected value of the inferred parameters and look at the resulting transition matrix $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters\n",
    "systm.set_theta(np.mean(samples_HMM, axis=0))\n",
    "\n",
    "# modify the transition matrix for nicer coloring\n",
    "mod_A = -1 * np.ones_like(systm.A)\n",
    "for key, nums in systm._idx_dict.items():\n",
    "    for i in nums:\n",
    "        mod_A[key, i] = systm.A[key, i]\n",
    "\n",
    "# plot the transition matrix\n",
    "fig, ax = plt.subplots(figsize=set_size(ratio=1.));\n",
    "\n",
    "h = ax.imshow(mod_A, cmap=halfGray_halfGreenToRed, vmin=-1., vmax=1.);\n",
    "ax.set_xticks(range(len(systm.state_list)));\n",
    "ax.set_xticklabels(systm.state_list, rotation=-90, fontsize=\"small\");\n",
    "ax.set_yticks(range(len(systm.state_list)));\n",
    "ax.set_yticklabels(systm.state_list, fontsize=\"small\");\n",
    "ax.tick_params(direction=\"out\")\n",
    "ax.grid(False)\n",
    "\n",
    "# label the non-zero entries with their probability in %\n",
    "for i in range(len(systm.state_list)):\n",
    "    for j in range(len(systm.state_list)):\n",
    "        if mod_A[i,j] > 0.:\n",
    "            ax.text(j,i, f\"{mod_A[i,j]*100:.1f}\", ha=\"center\", va=\"center\", \n",
    "                    color=\"white\", fontsize=\"x-small\")\n",
    "            \n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/transition_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/transition_matrix.svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution Plots\n",
    "\n",
    "We can also take a look at how this system evolves over the defined time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array containing the risk for each state...\n",
    "state_array = np.zeros(shape=(T_MAX+1, len(systm.state_list)), dtype=float)\n",
    "# ...weighted with the probability for that time\n",
    "state_array_weighted = np.zeros_like(state_array, dtype=float)\n",
    "state_array_summed = np.zeros_like(state_array, dtype=float)\n",
    "# starting state\n",
    "start = np.zeros(shape=(len(systm.state_list),))\n",
    "start[0] = 1.\n",
    "\n",
    "# manually evolving the system and storing all intermediate states\n",
    "time_prior = sp.stats.binom.pmf(np.arange(T_MAX+1), T_MAX, 0.4)\n",
    "for t,p in enumerate(time_prior):\n",
    "    state_array[t] = start\n",
    "    state_array_weighted[t] = p * start\n",
    "    state_array_summed[t] = np.ones(shape=(1,t+1)) @ state_array_weighted[:t+1]\n",
    "    state_array_summed[t] = state_array_summed[t] / np.sum(state_array_summed[t])\n",
    "    start = start @ systm.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these arrays define which states we need to marginalize over when we are \n",
    "# interested in a particual LNL's risk of involvement\n",
    "lnl_I_arr = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], \n",
    "                     dtype=float)\n",
    "lnl_II_arr = np.array([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1], \n",
    "                      dtype=float)\n",
    "lnl_III_arr = np.array([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1], \n",
    "                       dtype=float)\n",
    "lnl_IV_arr = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1], \n",
    "                      dtype=float)\n",
    "\n",
    "lnl_I = state_array @ lnl_I_arr\n",
    "lnl_II = state_array @ lnl_II_arr \n",
    "lnl_III = state_array @ lnl_III_arr \n",
    "lnl_IV = state_array @ lnl_IV_arr\n",
    "\n",
    "lnl_concat = np.vstack([lnl_I, lnl_II, lnl_III, lnl_IV])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marg_II = np.zeros(shape=(len(samples_HMM[::50])))\n",
    "marg_III = np.zeros(shape=(len(samples_HMM[::50])))\n",
    "marg_II_and_III = np.zeros(shape=(len(samples_HMM[::50])))\n",
    "marg_IV = np.zeros(shape=(len(samples_HMM[::50])))\n",
    "\n",
    "np.random.seed(SEED)\n",
    "for i, theta in enumerate(np.random.permutation(samples_HMM[::50])):\n",
    "    systm.set_theta(theta)\n",
    "    marg_II[i] = systm.risk(inv=np.array([None, 1, None, None]),\n",
    "                            obs={\"path\": np.array([None, None, None, None])},\n",
    "                            time_prior=time_prior) * 100\n",
    "    marg_III[i] = systm.risk(inv=np.array([None, None, 1, None]),\n",
    "                             obs={\"path\": np.array([None, None, None, None])},\n",
    "                             time_prior=time_prior) * 100\n",
    "    marg_II_and_III[i] = systm.risk(inv=np.array([None, 1, 1, None]),\n",
    "                                    obs={\"path\": np.array([None, None, None, None])},\n",
    "                                    time_prior=time_prior) * 100\n",
    "    marg_IV[i] = systm.risk(inv=np.array([None, None, None, 1]),\n",
    "                            obs={\"path\": np.array([None, None, None, None])},\n",
    "                            time_prior=time_prior) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marginalization for involvements of \"at least LNL x\" or \"only LNL x\" \n",
    "only_II = state_array[:,4]\n",
    "atleast_II = (state_array[:,4] + state_array[:,5] + state_array[:,6] \n",
    "              + state_array[:,7] + state_array[:,12] + state_array[:,13] \n",
    "              + state_array[:,14] + state_array[:,15])\n",
    "emp_II = 100 * np.sum(data[(\"path\", 'II')].to_numpy()) / 147\n",
    "\n",
    "only_III = state_array[:,2]\n",
    "atleast_III = (state_array[:,2] + state_array[:,3] + state_array[:,6] \n",
    "               + state_array[:,7] + state_array[:,10] + state_array[:,11] \n",
    "               + state_array[:,14] + state_array[:,15])\n",
    "emp_III = 100 * np.sum(data[(\"path\", 'III')].to_numpy()) / 147\n",
    "\n",
    "only_II_and_III = state_array[:,6]\n",
    "atleast_II_and_III = (state_array[:,6] + state_array[:,7] \n",
    "                      + state_array[:,14] + state_array[:,15])\n",
    "emp_II_and_III = 100 * len(data.loc[(data[(\"path\", 'II')]==1) \n",
    "                                    & (data[(\"path\", 'III')]==1)].to_numpy()) / 147\n",
    "\n",
    "only_IV = state_array[:,1]\n",
    "atleast_IV = (state_array[:,1] + state_array[:,3] + state_array[:,5] \n",
    "              + state_array[:,7] + state_array[:,9] + state_array[:,11] \n",
    "              + state_array[:,13] + state_array[:,15])\n",
    "emp_IV = 100 * np.sum(data[(\"path\", 'IV')].to_numpy()) / 147\n",
    "\n",
    "# and now for one complicated plot...\n",
    "fig = plt.figure(figsize=set_size(width=\"full\", ratio=2*1.61));\n",
    "spec = gs.GridSpec(ncols=3, nrows=1, figure=fig, width_ratios=[1., 1., 0.3]);\n",
    "\n",
    "# leftmost subplot\n",
    "ax = fig.add_subplot(spec[0,0])\n",
    "ax.plot(range(len(time_prior)), 100*only_II, 'o-', \n",
    "        label=r\"$\\xi_5=[0\\ 1\\ 0\\ 0]$\");\n",
    "ax.plot(range(len(time_prior)), 100*only_III, 'o-', \n",
    "        label=r\"$\\xi_3=[0\\ 0\\ 1\\ 0]$\");\n",
    "ax.plot(range(len(time_prior)), 100*only_II_and_III, 'o-', \n",
    "        label=r\"$\\xi_7=[0\\ 1\\ 1\\ 0]$\");\n",
    "ax.plot(range(len(time_prior)), 100*only_IV, 'o-', \n",
    "        label=r\"$\\xi_2=[0\\ 0\\ 0\\ 1]$\");\n",
    "ax.set_xlabel(\"time step $t$\");\n",
    "ax.set_ylim(ymax=50);\n",
    "ax.set_ylabel(\"Risk [%]\");\n",
    "ax.legend();\n",
    "\n",
    "# middle subplot\n",
    "ax = fig.add_subplot(spec[0,1])\n",
    "ax.plot(range(len(time_prior)), 100*atleast_II, 'o-', \n",
    "        label=\"lvl II involved\");\n",
    "ax.plot(range(len(time_prior)), 100*atleast_III, 'o-', \n",
    "        label=\"lvl III involved\");\n",
    "ax.plot(range(len(time_prior)), 100*atleast_II_and_III, 'o-', \n",
    "        label=\"lvl II & III involved\");\n",
    "ax.plot(range(len(time_prior)), 100*atleast_IV, 'o-', \n",
    "        label=\"lvl IV involved\");\n",
    "ax.set_xlabel(\"time step $t$\");\n",
    "ax.set_ylim(ymax=100);\n",
    "ax.legend();\n",
    "\n",
    "# rightmost subplot\n",
    "ax = fig.add_subplot(spec[0,2], sharey=ax);\n",
    "plt.setp(ax.get_yticklabels(), visible=False);\n",
    "ax.set_xticks([0, 1, 2, 3]);\n",
    "ax.set_xticklabels([\"II\", \"III\", \"II & III\", \"IV\"], rotation=-45);\n",
    "\n",
    "violin = ax.violinplot(marg_II, positions=[0]);\n",
    "violin[\"bodies\"][0].set_color(usz_blue);\n",
    "violin[\"cbars\"].set_color(usz_blue);\n",
    "ax.axhline(emp_II, color=usz_blue, ls=\"--\");\n",
    "\n",
    "violin = ax.violinplot(marg_III, positions=[1]);\n",
    "violin[\"bodies\"][0].set_color(usz_orange);\n",
    "violin[\"cbars\"].set_color(usz_orange);\n",
    "ax.axhline(emp_III, color=usz_orange, ls=\"--\");\n",
    "\n",
    "violin = ax.violinplot(marg_II_and_III, positions=[2]);\n",
    "violin[\"bodies\"][0].set_color(usz_red);\n",
    "violin[\"cbars\"].set_color(usz_red);\n",
    "ax.axhline(emp_II_and_III, color=usz_red, ls=\"--\");\n",
    "\n",
    "violin = ax.violinplot(marg_IV, positions=[3]);\n",
    "violin[\"bodies\"][0].set_color(usz_green);\n",
    "violin[\"cbars\"].set_color(usz_green);\n",
    "ax.axhline(emp_IV, color=usz_green, ls=\"--\");\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/HMM_evolution.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/HMM_evolution.svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    1,2,\n",
    "    sharey=True,\n",
    "    figsize=set_size(width=\"full\", ratio=1.8),\n",
    "    constrained_layout=False,\n",
    "    gridspec_kw={\n",
    "        \"width_ratios\": [1., 0.35],\n",
    "        \"wspace\": 0.05,\n",
    "        \"height_ratios\": [1.]\n",
    "    }\n",
    ")\n",
    "\n",
    "ax[0].set_title(\"Probabilities [%] of states at different time steps\")\n",
    "ax[0].imshow(state_array, cmap=green_to_red);\n",
    "for i in range(len(time_prior)):\n",
    "    for j in range(len(systm.state_list)):\n",
    "        if np.around(state_array[i,j]*100,1) >= 1.:\n",
    "            ax[0].text(j,i, f\"{state_array[i,j]*100:.1f}\", \n",
    "                       ha=\"center\", va=\"center\", \n",
    "                       color=\"white\", fontsize=\"xx-small\")\n",
    "ax[0].set_xticks(range(len(systm.state_list)))\n",
    "ax[0].set_xticklabels(systm.state_list, rotation=45);\n",
    "ax[0].set_ylabel(\"time step $t$\")\n",
    "ax[0].set_xlabel(r\"state $\\xi$\")\n",
    "ax[0].grid(False)\n",
    "ax0_pos = ax[0].get_position()\n",
    "\n",
    "ax[1].set_title(\"Time prior (PDF)\")\n",
    "ax[1].plot(time_prior, range(len(time_prior)), \"o-\")\n",
    "ax1_pos = ax[1].get_position()\n",
    "ax[1].set_position(\n",
    "    [ax1_pos.x0,\n",
    "     ax0_pos.y0,\n",
    "     ax1_pos.x1 - ax1_pos.x0,\n",
    "     ax0_pos.y1 - ax0_pos.y0]\n",
    ")\n",
    "plt.setp(ax[1].get_yticklabels(), visible=False);\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/HMM_evo_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/HMM_evo_matrix.svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Cross-)Validation\n",
    "\n",
    "As an attempt to validate the model with the limited data we have, I'll start by simply comparing the prevalence of certain patterns of involvement to the prediction of the model for the respective state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thin = 100\n",
    "np.random.seed(SEED)\n",
    "\n",
    "risks = np.zeros(shape=(len(systm.obs_list), len(samples_HMM[::thin])), dtype=float)\n",
    "\n",
    "for i, sample in enumerate(np.random.permutation(samples_HMM[::thin])):\n",
    "    for j, obs in enumerate(systm.obs_list):\n",
    "        systm.set_theta(sample)\n",
    "        risks[j,i] = systm.risk(inv=obs, obs={\"path\": [None, None, None, None]}, \n",
    "                                time_prior=time_prior_dict[\"early\"], mode=\"HMM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurences = np.zeros(shape=(len(systm.obs_list),), dtype=int)\n",
    "for i,obs in enumerate(systm.obs_list):\n",
    "    if np.sum(systm.C_dict[\"early\"][i]) == 0:\n",
    "        occurences[i] = 0\n",
    "    else:\n",
    "        idx = np.argmax(systm.C_dict[\"early\"][i])\n",
    "        occurences[i] = systm.f_dict[\"early\"][idx]\n",
    "\n",
    "validation_df = pd.DataFrame({\"state\": [str(obs) for obs in systm.obs_list], \n",
    "                              \"occurence\": occurences, \n",
    "                              \"percentage\": 100 * occurences / np.sum(occurences), \n",
    "                              \"prediction\": 100 * np.mean(risks, axis=1)})\n",
    "validation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing I could do is to randomly split the dataset into two parts and train the model on the first half, then generating the above table for the second half. Then do it the other way around and/or split the whole dataset randomly again. What's now left to find is a measure of the difference *prevalence* vs *predicted risk*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to Bayesian Network\n",
    "\n",
    "To be able to compare our results to the Bayesian network by [Pouymayou et al.](#pouymayou) we needed to recreate it using the same sampler. To this end, the `lymph` package also supports computing the Bayesian network likelihood for a given graph and observational modality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract representation of the lymphatic network\n",
    "graph = {'T'  : ['I', 'II', 'III', 'IV'], \n",
    "         'I'  : ['II'], \n",
    "         'II' : ['III'], \n",
    "         'III': ['IV'], \n",
    "         'IV' : []}\n",
    "\n",
    "systm = lymph.System(graph=graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning (BN)\n",
    "\n",
    "All that is different to the learning round [above](#Learning-(HMM)) is that one has to specify the `mode` to be `\"BN\"` instead of `\"HMM\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define specificity and sensitivity for diagnostic modalities\n",
    "spsn_dict = {\"path\": [1., 1.]}\n",
    "\n",
    "# generate C matrix from data\n",
    "systm.load_data(data, spsn_dict=spsn_dict, mode=\"BN\")\n",
    "\n",
    "# check if likelihood works\n",
    "assert systm.likelihood(np.random.uniform(size=(7,)), mode=\"BN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the sampler\n",
    "ndim, nwalker, nstep, burnin = 7, 200, 2000, 1000\n",
    "moves = [(emcee.moves.DEMove(), 0.8), (emcee.moves.DESnookerMove(), 0.2)]\n",
    "\n",
    "if DRAW_SAMPLES:\n",
    "    # starting point\n",
    "    np.random.seed(SEED)\n",
    "    theta0 = np.random.uniform(low=0., high=1., size=(nwalker,ndim))\n",
    "\n",
    "    # the actual sampling round\n",
    "    if __name__ == \"__main__\":\n",
    "        with Pool() as pool:\n",
    "            sampler = emcee.EnsembleSampler(nwalker, ndim, systm.likelihood, \n",
    "                                            kwargs={\"mode\": \"BN\"},\n",
    "                                            moves=moves, pool=pool)\n",
    "            sampler.run_mcmc(theta0, nstep, progress=True)\n",
    "\n",
    "    # extracting 200,000 of the 400,000 samples\n",
    "    samples_BN = sampler.get_chain(flat=True, discard=burnin)\n",
    "\n",
    "    # saving the sampled data to disk for later convenience\n",
    "    np.save(\"./samples/BN.npy\", samples_BN)\n",
    "    \n",
    "else:\n",
    "    # loading in case we don't want to draw all the samples again\n",
    "    samples_BN = np.load(\"./samples/BN.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DRAW_SAMPLES:\n",
    "    # check acceptance faction of the sampler to get an indication on whether sth\n",
    "    # went wrong or not\n",
    "    ar = np.mean(sampler.acceptance_fraction)\n",
    "    print(f\"the BN sampler accepted {ar * 100 :.2f} % of samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [r\"$b_1$\", r\"$b_2$\", r\"$b_3$\", r\"$b_4$\", \n",
    "          r\"$t_{12}$\", r\"$t_{23}$\", r\"$t_{34}$\"]\n",
    "\n",
    "fig = plt.figure(figsize=set_size(width=\"full\", ratio=1))\n",
    "\n",
    "# using the corner plot package\n",
    "corner.corner(\n",
    "    samples_BN,\n",
    "    labels=labels,\n",
    "    smooth=True,\n",
    "    fig=fig,\n",
    "    color=usz_green,\n",
    "    hist_kwargs={'histtype': 'stepfilled', 'color': usz_green},\n",
    "    **{\n",
    "        \"plot_datapoints\": False,\n",
    "        \"plot_density\": True,\n",
    "        \"no_fill_contours\": True,\n",
    "        \"contour_kwargs\": {\"colors\": \"black\"},\n",
    "        \"levels\": np.array([0.2, 0.5, 0.8])\n",
    "    },\n",
    "    show_titles=True,\n",
    "    title_kwargs={\"fontsize\": \"medium\"}\n",
    ");\n",
    "\n",
    "axes = fig.get_axes()\n",
    "for ax in axes:\n",
    "    ax.grid(False)\n",
    "    \n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/corner_BN.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/corner_BN.svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Predictions\n",
    "\n",
    "We can now compute distributions over risks using both the HMM model, as well as the BN.\n",
    "\n",
    "### Evolving beyond \"early\" T-stage\n",
    "\n",
    "We can use the parameters inferred from the early T-stage dataset and use time priors that expect to see a patient's diagnose later on to estimate how risks of involvement might increase over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract representation of the lymphatic network\n",
    "graph = {'T'  : ['I', 'II', 'III', 'IV'], \n",
    "         'I'  : ['II'], \n",
    "         'II' : ['III'], \n",
    "         'III': ['IV'], \n",
    "         'IV' : []}\n",
    "\n",
    "tst_systm = lymph.System(graph=graph)\n",
    "\n",
    "# set specificity & sensitivity of diagnostic modality (here CT) manually\n",
    "spsn_dict = {\"CT\": [0.76, 0.81]}\n",
    "tst_systm.set_modalities(spsn_dict=spsn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time priors\n",
    "time_prior_dict = {}\n",
    "time_prior_dict['early'] = sp.stats.binom.pmf(np.arange(T_MAX+1), T_MAX, 0.4)\n",
    "time_prior_dict['mid'] = sp.stats.binom.pmf(np.arange(T_MAX+1), T_MAX, 0.55)\n",
    "time_prior_dict['late'] = sp.stats.binom.pmf(np.arange(T_MAX+1), T_MAX, 0.7)\n",
    "\n",
    "# what do we want to know, what do we know?\n",
    "inv = np.array([None, None, 1, None])  # we're interested in the risk of LNL 3\n",
    "# our observation is that lvl 2 is involved\n",
    "obs = {\"CT\": np.array([0, 1, 0, 0])}\n",
    "\n",
    "thin = 50\n",
    "# risk for HMM and different \"T-stages\"\n",
    "early = []\n",
    "mid = []\n",
    "late = []\n",
    "np.random.seed(SEED)\n",
    "for sample in np.random.permutation(samples_HMM)[::thin]:\n",
    "    tst_systm.set_theta(sample)\n",
    "    early.append(tst_systm.risk(inv=inv, obs=obs, \n",
    "                                time_prior=time_prior_dict[\"early\"], \n",
    "                                mode=\"HMM\"))\n",
    "    mid.append(tst_systm.risk(inv=inv, obs=obs, \n",
    "                              time_prior=time_prior_dict[\"mid\"], \n",
    "                              mode=\"HMM\"))\n",
    "    late.append(tst_systm.risk(inv=inv, obs=obs, \n",
    "                               time_prior=time_prior_dict[\"late\"], \n",
    "                               mode=\"HMM\"))\n",
    "\n",
    "# risk for BN\n",
    "bn = []\n",
    "np.random.seed(SEED)\n",
    "for sample in np.random.permutation(samples_BN)[::thin]:\n",
    "    tst_systm.set_theta(sample)\n",
    "    bn.append(tst_systm.risk(inv=inv, obs=obs, mode=\"BN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 35, 50)\n",
    "r = (0, 35)\n",
    "fig, ax = plt.subplots(figsize=set_size())\n",
    "\n",
    "ax.hist(np.asarray(early)*100., bins=bins, density=True, \n",
    "        histtype=\"stepfilled\", color=usz_green, label=\"$p = 0.4$\");\n",
    "ax.hist(np.asarray(mid)*100., bins=bins, density=True, alpha=0.8, \n",
    "        histtype=\"stepfilled\", color=usz_orange, label=\"$p = 0.55$\");\n",
    "ax.hist(np.asarray(late)*100., bins=bins, density=True, alpha=0.8, \n",
    "        histtype=\"stepfilled\", color=usz_red, label=\"$p = 0.7$\");\n",
    "ax.hist(np.asarray(bn)*100., bins=bins, histtype=\"step\", density=True, \n",
    "        color=usz_blue, label=\"Bayesian network\");\n",
    "\n",
    "ax.set_xlim(r)\n",
    "ax.set_xlabel(\"risk $R$ [%]\");\n",
    "ax.set_ylabel(r\"$p(R)$\");\n",
    "ax.tick_params();\n",
    "ax.legend();\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/HMM_risk_increaseP.png\", dpi=300, \n",
    "                bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/HMM_risk_increaseP.svg\", \n",
    "                bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to [Pouymayou et al.](#pouymayou)\n",
    "\n",
    "Now we will compare how the sampled HMM's, sampled BN's and maximum likelihood BN's risk predictions compare to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim, nwalker, nstep, burnin = 7, 200, 2000, 1000\n",
    "thin = 50\n",
    "\n",
    "# what do we want to know?\n",
    "inv = np.array([[1   , None, None, None],\n",
    "                [None, 1   , None, None],\n",
    "                [None, None, 1   , None],\n",
    "                [None, None, None, 1   ]])\n",
    "\n",
    "# what do we know?\n",
    "obs = np.array([[0, 0, 0, 0],\n",
    "                [0, 1, 0, 0],\n",
    "                [0, 1, 1, 0],\n",
    "                [0, 0, 1, 0]])\n",
    "\n",
    "# risk for HMM and two different \"T-stages\" (early and late)\n",
    "np.random.seed(SEED)\n",
    "hmm_risk = np.zeros(shape=(4,4,(nstep-burnin)*nwalker//thin))\n",
    "for i, sample in enumerate(np.random.permutation(samples_HMM)[::thin]):\n",
    "    tst_systm.set_theta(sample)\n",
    "    for k in range(4):\n",
    "        for l in range(4):\n",
    "            hmm_risk[k,l,i] = tst_systm.risk(inv=inv[k], obs={\"CT\": obs[l]}, \n",
    "                                             time_prior=time_prior_dict[\"early\"], \n",
    "                                             mode=\"HMM\")\n",
    "\n",
    "# risk for BN\n",
    "np.random.seed(SEED)\n",
    "bn_risk = np.zeros(shape=(4,4,(nstep-burnin)*nwalker//thin))\n",
    "for i, sample in enumerate(np.random.permutation(samples_BN)[::thin]):\n",
    "    tst_systm.set_theta(sample)\n",
    "    for k in range(4):\n",
    "        for l in range(4):\n",
    "            bn_risk[k,l,i] = tst_systm.risk(inv=inv[k], obs={\"CT\": obs[l]}, \n",
    "                                            mode=\"BN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=set_size(width=\"full\"), constrained_layout=False)\n",
    "spec = gs.GridSpec(ncols=4, nrows=4, figure=fig, wspace=0.1)\n",
    "\n",
    "lvls = [\"I\", \"II\", \"III\", \"IV\"]\n",
    "txt = [\"Ø\", \"II\", \"II & III\", \"III\"]\n",
    "risk_map = LinearSegmentedColormap.from_list(\"risk_map\", [usz_green, \n",
    "                                                          usz_gray, \n",
    "                                                          usz_red, \n",
    "                                                          usz_red], N=256)\n",
    "n_bins = 25\n",
    "\n",
    "for i in range(4):\n",
    "    if (i == 0) or (i == 3):\n",
    "        bins = np.linspace(0., 12., 30)\n",
    "        for j in range(4):\n",
    "            if j == 0:\n",
    "                ax = fig.add_subplot(spec[i,j])\n",
    "                ax.set_ylabel(f\"{lvls[i]}\");\n",
    "            else:\n",
    "                ax = fig.add_subplot(spec[i,j], sharey=ax)\n",
    "                plt.setp(ax.get_yticklabels(), visible=False)\n",
    "                \n",
    "            ax.set_xlim(bins[0], bins[-1])\n",
    "            \n",
    "            tmp_mean = np.mean(hmm_risk[i,j])\n",
    "            hmm_color = risk_map(tmp_mean)\n",
    "            ax.axvline(pouymayou_MLrisk[i,j]*100., \n",
    "                       color=usz_orange, label=\"Pouymayou et al\");\n",
    "            _, bins, _ = ax.hist(hmm_risk[i,j]*100., bins=bins, \n",
    "                                 histtype=\"stepfilled\", density=True, \n",
    "                                 color=hmm_color) #, label=\"HMM sampling\");\n",
    "            ax.hist(bn_risk[i,j]*100., bins=bins, density=True, \n",
    "                    histtype=\"step\", label=\"BN sampling\", color=usz_blue);\n",
    "            ax.tick_params(labelsize=\"xx-small\")\n",
    "            \n",
    "            if i == 0:\n",
    "                ax.set_title(f\"{txt[j]}\", \n",
    "                             fontsize=\"medium\", fontweight=\"normal\");\n",
    "            else:\n",
    "                ax.set_xlabel(\"risk [%]\");\n",
    "                \n",
    "    else:\n",
    "        ax = fig.add_subplot(spec[i,:])\n",
    "        bins = np.linspace(0., 100., 150)\n",
    "        ax.set_xlim(bins[0], bins[-1])\n",
    "        ax.set_ylabel(f\"{lvls[i]}\");\n",
    "        \n",
    "        for j in range(4):\n",
    "            tmp_mean = np.mean(hmm_risk[i,j])\n",
    "            hmm_color = risk_map(tmp_mean)\n",
    "            ax.axvline(pouymayou_MLrisk[i,j]*100., \n",
    "                       color=usz_orange, label=\"Pouymayou et al\");\n",
    "            n, bins, _ = ax.hist(hmm_risk[i,j]*100., bins=bins, \n",
    "                                 histtype=\"stepfilled\", density=True, \n",
    "                                 color=hmm_color) #, label=\"HMM sampling\");\n",
    "            ax.hist(bn_risk[i,j]*100., bins=bins, density=True, \n",
    "                    histtype=\"step\", label=\"BN sampling\", color=usz_blue, \n",
    "                    linestyle='-');\n",
    "            ax.set_xticks(np.linspace(0,100, 11))\n",
    "            ax.tick_params(labelsize=\"xx-small\")\n",
    "            \n",
    "            if ((i == 2) and (j == 0)):\n",
    "                ax.text(x=100*pouymayou_MLrisk[i,j]-4*(bins[1]-bins[0]), \n",
    "                        y=0.35, \n",
    "                        s=txt[j])\n",
    "            elif ((i == 1) and (j == 2)):\n",
    "                ax.text(x=100*pouymayou_MLrisk[i,j]+2.5*(bins[1]-bins[0]), \n",
    "                        y=0.2, \n",
    "                        s=txt[j])\n",
    "            else:\n",
    "                ax.text(x=100*pouymayou_MLrisk[i,j]+(bins[1]-bins[0]), \n",
    "                        y=np.max(n)+0.05, \n",
    "                        s=txt[j])\n",
    "                \n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/HMM_BN_risk_comparison.png\", dpi=300, \n",
    "                bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/HMM_BN_risk_comparison.svg\", \n",
    "                bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the good agreement between the different ways of computing the risk using different models and different inference techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simultaneous Learning\n",
    "If we learn both the system's parameters AND the center of the time prior at the same time. But the naive way just leads to overfitting and very unrealistic combinations of parameters. So, what we are doing here is fixing the time prior for the early stage learning (and use the sanguineti data with chosen N0-ratio) and learn the probability rates along with the time-prior for the late stage (where the data only consists of N0 / N+ patients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract representation of the lymphatic network\n",
    "graph = {'T'  : ['I', 'II', 'III', 'IV'], \n",
    "         'I'  : ['II'], \n",
    "         'II' : ['III'], \n",
    "         'III': ['IV'], \n",
    "         'IV' : []}\n",
    "\n",
    "systm = lymph.System(graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define specificity and sensitivity for diagnostic modalities\n",
    "spsn_dict = {\"path\": [1., 1.]}\n",
    "\n",
    "# generate FIRST PART of C matrix...\n",
    "systm.load_data(data, t_stage=[\"early\"], spsn_dict=spsn_dict, mode=\"HMM\")\n",
    "\n",
    "# ...and SECOND PART\n",
    "systm.C_dict['late'] = np.zeros(shape=(len(systm.obs_list),2), dtype=int)\n",
    "systm.f_dict['late'] = np.zeros(shape=(2,), dtype=int)\n",
    "\n",
    "n_late = np.sum(systm.f_dict['early'])\n",
    "# N0 of late stage\n",
    "systm.C_dict['late'][0,0] = 1\n",
    "systm.f_dict['late'][0] = int(n_late * (1 - (1. / 1.2)))  # 20% N0 for late T-stage\n",
    "# N+ of late stage\n",
    "systm.C_dict['late'][(np.sum(systm.obs_list, axis=1) > 0),1] = 1\n",
    "systm.f_dict['late'][1] = int(n_late / 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = np.random.uniform(size=(8,))\n",
    "assert systm.combined_likelihood(theta0, t_stage=['early', 'late'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim, nwalker, nstep, burnin = 8, 200, 2000, 1000\n",
    "np.random.seed(SEED)\n",
    "theta0 = np.random.uniform(low=0., high=1., size=(nwalker,ndim))\n",
    "moves = [(emcee.moves.DEMove(), 0.8), (emcee.moves.DESnookerMove(), 0.2)]\n",
    "\n",
    "if DRAW_SAMPLES:\n",
    "    if __name__ == \"__main__\":\n",
    "        with Pool() as pool:\n",
    "            sampler = emcee.EnsembleSampler(nwalker, ndim, systm.combined_likelihood, \n",
    "                                            args=[['early', 'late']], \n",
    "                                            moves=moves, pool=pool)\n",
    "            sampler.run_mcmc(theta0, nstep, progress=True)\n",
    "\n",
    "    # extracting 200,000 of the 400,000 samples\n",
    "    samples_simultaneous = sampler.get_chain(flat=True, discard=burnin)\n",
    "\n",
    "    # saving the sampled data to disk for later convenience\n",
    "    np.save(\"./samples/simultaneous.npy\", samples_simultaneous)\n",
    "    \n",
    "else:\n",
    "    # loading in case we don't want to draw all the samples again\n",
    "    samples_simultaneous = np.load(\"./samples/simultaneous.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DRAW_SAMPLES:\n",
    "    # check acceptance faction of the sampler to get an indication on whether sth\n",
    "    # went wrong or not\n",
    "    ar = np.mean(sampler.acceptance_fraction)\n",
    "    print(f\"the simultaneous sampler accepted {ar * 100 :.2f} % of samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [r\"$\\tilde{b}_1$\", r\"$\\tilde{b}_2$\", \n",
    "          r\"$\\tilde{b}_3$\", r\"$\\tilde{b}_4$\", \n",
    "          r\"$\\tilde{t}_{12}$\", r\"$\\tilde{t}_{23}$\", \n",
    "          r\"$\\tilde{t}_{34}$\", r\"$p$\"]\n",
    "\n",
    "fig = plt.figure(figsize=set_size(width=\"full\", ratio=1))\n",
    "corner.corner(\n",
    "    samples_simultaneous,\n",
    "    labels=labels,\n",
    "    smooth=True,\n",
    "    fig=fig,\n",
    "    color=usz_blue,\n",
    "    hist_kwargs={'histtype': 'stepfilled', 'color': usz_blue},\n",
    "    **{\n",
    "        \"plot_datapoints\": False,\n",
    "        \"no_fill_contours\": True,\n",
    "        \"plot_density\": True,\n",
    "        \"contour_kwargs\": {\"colors\": \"black\"},\n",
    "        \"levels\": np.array([0.2, 0.5, 0.8])\n",
    "    },\n",
    "    show_titles=True,\n",
    "    title_kwargs={\"fontsize\": \"medium\"}\n",
    ");\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/corner_simultaneous.png\", dpi=300, \n",
    "                bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/corner_simultaneous.svg\", \n",
    "                bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_late_p = np.mean(samples_simultaneous[:,7])\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=set_size(width=\"full\", ratio=1.61**2))\n",
    "\n",
    "ax[0].axvline(0.4, color=usz_blue, linewidth=2, \n",
    "              label=r\"$p_{\\mathrm{early}}$ (fixed)\");\n",
    "ax[0].hist(samples_simultaneous[:,7], bins=40, density=True, \n",
    "           color=usz_red, histtype=\"stepfilled\", \n",
    "           label=r\"$p_{\\mathrm{late}}$\");\n",
    "ax[0].set_xlabel(\"Binomial parameter $p$\");\n",
    "# ax[0].set_ylabel(r\"$p\\left(\\theta_p^T\\right)$\", fontsize=\"large\");\n",
    "ax[0].legend();\n",
    "\n",
    "t = np.arange(T_MAX+1)\n",
    "ax[1].plot(t, sp.stats.binom.pmf(t, T_MAX, 0.4), 'o-', \n",
    "           label=r\"$p_{\\mathrm{early}}$ (fixed)\");\n",
    "ax[1].plot(t, sp.stats.binom.pmf(t, T_MAX, mean_late_p), 'o-', color=usz_red, \n",
    "           label=r\"$\\mathbb{E}[p_{\\mathrm{late}}]$\");\n",
    "\n",
    "ax[1].legend();\n",
    "ax[1].set_xlabel(\"Time step $t$\");\n",
    "ax[1].set_ylabel(r\"$p_T(t)$\");\n",
    "ax[1].set_xlim([1,T_MAX]);\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/simultaneous_learnedP.png\", dpi=300, \n",
    "                bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/simultaneous_learnedP.svg\", \n",
    "                bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can again compare risk predictions for different T-stages of disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitivity and specificity from [2]\n",
    "graph = {'T'  : ['I', 'II', 'III', 'IV'], \n",
    "         'I'  : ['II'], \n",
    "         'II' : ['III'], \n",
    "         'III': ['IV'], \n",
    "         'IV' : []}\n",
    "\n",
    "tst_systm = lymph.System(graph=graph)\n",
    "tst_systm.set_modalities(spsn_dict={\"CT\": [0.76, 0.81]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "nsubset = 20000\n",
    "subset = samples_simultaneous[np.random.randint(low=0, \n",
    "                                                high=(nstep-burnin)*nwalker, \n",
    "                                                size=nsubset)]\n",
    "risk_III = np.zeros(shape=(4, nsubset))\n",
    "risk_IV = np.zeros(shape=(4, nsubset))\n",
    "\n",
    "inv_III = np.array([None, None, 1, None])\n",
    "obs_III = np.array([[0, 0, 0, 0],   # no involvement in lvl II observed\n",
    "                    [0, 1, 0, 0]])  # involvement observed\n",
    "\n",
    "inv_IV = np.array([None, None, None, 1])\n",
    "obs_IV = np.array([[0, 0, 0, 0], \n",
    "                   [0, 1, 1, 0]])\n",
    "\n",
    "for i, th in enumerate(subset):\n",
    "    tst_systm.set_theta(th[:7])\n",
    "    prior = np.vstack([sp.stats.binom.pmf(np.arange(T_MAX+1), T_MAX, 0.4), \n",
    "                       sp.stats.binom.pmf(np.arange(T_MAX+1), T_MAX, th[7])])\n",
    "    \n",
    "    for k in range(4):\n",
    "        risk_III[k,i] = tst_systm.risk(inv=inv_III, \n",
    "                                       obs={\"CT\": obs_III[k % 2]}, \n",
    "                                       time_prior=prior[k // 2])\n",
    "        risk_IV[k,i] = tst_systm.risk(inv=inv_IV, \n",
    "                                      obs={\"CT\": obs_IV[k % 2]}, \n",
    "                                      time_prior=prior[k // 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, \n",
    "                       figsize=set_size(width=\"full\", ratio=2*1.5))\n",
    "kwargs = [{\"histtype\": \"stepfilled\", \"alpha\": 0.5}, \n",
    "          {\"histtype\": \"step\", \"linewidth\": 1.5}]\n",
    "colors = [usz_blue, usz_orange]\n",
    "time_label = [\"early\", \"late\"]\n",
    "inv_III_label = [\"no observed involvement\", \n",
    "                 \"only LNL II observed involved\"]\n",
    "inv_IV_label = [\"no observed involvement\", \n",
    "                \"LNL II & III observed involved\"]\n",
    "\n",
    "for k in range(4):\n",
    "    bins = np.linspace(0., 25., 50)\n",
    "    ax[0].hist(100*risk_III[k], bins=bins, density=True, \n",
    "               color=colors[k // 2], **kwargs[k % 2], \n",
    "               label=f\"{time_label[k // 2]} | {inv_III_label[k % 2]}\")\n",
    "    ax[0].set_xlabel(\"risk $R$ [%]\");\n",
    "    ax[0].set_ylabel(r\"$p(R)$\");\n",
    "    ax[0].set_xlim([bins[0], bins[-1]])\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title(\"III\")\n",
    "    \n",
    "    bins = np.linspace(0., 15., 50)\n",
    "    ax[1].hist(100*risk_IV[k], bins=bins, density=True, \n",
    "               color=colors[k // 2], **kwargs[k % 2], \n",
    "               label=f\"{time_label[k // 2]} | {inv_IV_label[k % 2]}\")\n",
    "    ax[1].set_xlabel(\"risk $R$ [%]\");\n",
    "    ax[1].set_xlim([bins[0], bins[-1]]);\n",
    "    ax[1].legend()\n",
    "    ax[1].set_title(\"IV\")\n",
    "    \n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/simultaneous_risk.png\", dpi=300, \n",
    "                bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/simultaneous_risk.svg\", \n",
    "                bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. <a class=\"anchor\" id=\"sanguineti\"></a>Sanguineti Giuseppe [et al.] Defining the risk of involvement for each neck nodal level in patients with early T-stage node-positive oropharyngeal carcinoma. [Journal] // International Journal of Radiation Oncology Biology Physics. - 2008. - 5 : Vol. 74. - pp. 1356-1364.\n",
    "2. <a class=\"anchor\" id=\"pouymayou\"></a>Pouymayou Bertrand [et al.] A Bayesian network model of lymphatic tumor progression for personalized elective CTV definition in head and neck cancers [Journal] // Physics in Medicine and Biology. - 2019. - 16 : Vol. 64. - p. 165003."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### t-Dependence of Rates\n",
    "\n",
    "Since the HMM-formalism has more parameters than the BN through its time prior, we expect the system to be somewhat overdetermined. In our case this means that we can basically choose an arbitrary number of time steps and the base and transition probability rates will essentially adapt to our choice. To see *how* the rates depend on the time prior's length we'll look at a simplistic example:\n",
    "\n",
    "For the simplest example, the time prior $p_T(t,T)=1 / T$ is uniform and we're only looking at a system with one node that has empirically an involvement probability of $p^*$ (e.g. $0.4$), the base probability rate $p$ must become smaller, as the length of the uniform prior increases. More formally,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p^* \n",
    "&= \\sum_{t=1}^{T}{\\frac{1}{T}\\begin{pmatrix} 1 & 0 \\end{pmatrix}\\begin{bmatrix} 1-p & p \\\\ 0 & 1 \\end{bmatrix}^t\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}} \\\\\n",
    "&= \\sum_{t=1}^{T}{\\frac{1}{T}\\begin{pmatrix} 1 & 0 \\end{pmatrix}\\begin{bmatrix} (1-p)^t & 1-(1-p)^t \\\\ 0 & 1 \\end{bmatrix}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}} \\\\\n",
    "&= \\sum_{t=1}^{T}{\\frac{1}{T}\\sum_{k=1}^{t}{{t\\choose k}p^k(1-p)^{t-k}}}\n",
    "= \\sum_{t=1}^{T}{\\frac{1}{T}\\left[ 1 - (1-p)^t \\right]} \\\\\n",
    "&= 1 - \\frac{1}{T}\\sum_{t=1}^{T}{(1-p)^t}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let's write $q = 1 - p$ and then the sum as\n",
    "\n",
    "$$\n",
    "s = \\sum_{t=1}^{T}{q^t} = q + q^2 + \\ldots + q^T\n",
    "$$\n",
    "\n",
    "hence\n",
    "\n",
    "$$\n",
    "q\\cdot s = q^2 + q^3 + \\ldots + q^{T+1}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "s - qs &= q - q^{T+1} \\\\\n",
    "\\Rightarrow\\, s(1 - q) &= q(1 - q^T) \\\\\n",
    "\\Rightarrow\\, s &= \\frac{q(1 - q^T)}{1 - q}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So we can write the empirical probability $p^*$ as\n",
    "\n",
    "$$\n",
    "p^* = 1 - \\frac{q(1 - q^T)}{T(1 - q)} = 1 - \\frac{(1-p)(1 - (1-p)^T)}{Tp}\n",
    "$$\n",
    "\n",
    "This cannot be solved analytically for $p$ in the case of arbitrary $T$, but it is easy to find numerical solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_star = 0.4\n",
    "max_prior_length = 15\n",
    "\n",
    "Ts = np.linspace(1, max_prior_length, max_prior_length, dtype=int)\n",
    "f = lambda p,t,p_star : 1 - (1 - p)*(1 - (1-p)**t) / (t*p) - p_star\n",
    "\n",
    "sols = []\n",
    "for t in Ts:\n",
    "    # find roots of f numerically\n",
    "    sols.append(sp.optimize.root_scalar(f, args=(t,p_star), bracket=[0.001, 0.999]).root)\n",
    "    \n",
    "# plot the computed solutions\n",
    "fig, ax = plt.subplots(figsize=set_size())\n",
    "ax.plot(Ts, sols, 'o--');\n",
    "ax.set_xlim([0, max_prior_length+1]);\n",
    "ax.set_xticks(Ts[::2]);\n",
    "ax.set_xlabel(\"length of prior $T$\");\n",
    "ax.set_ylabel(r\"solution for $\\tilde{b}_1$\");\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/simple_rate_decay.png\", dpi=300, \n",
    "                bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/simple_rate_decay.svg\", \n",
    "                bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if this overly simplified model applies to the full HMM as well, we trained our model for 15 different uniform time priors and plotted the learned mean parameters. We also compared the risk predictions using these different time priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling parameters\n",
    "ndim, nwalker, nstep, burnin = 7, 200, 2000, 1000\n",
    "moves = [(emcee.moves.DEMove(), 0.8), (emcee.moves.DESnookerMove(), 0.2)]\n",
    "\n",
    "# starting point\n",
    "np.random.seed(SEED)\n",
    "theta0 = np.random.uniform(low=0., high=1., size=(nwalker,ndim))\n",
    "prior_dict = {}\n",
    "\n",
    "samples_multiT_HMM = np.zeros(shape=(max_prior_length, \n",
    "                                     nwalker*(nstep-burnin), \n",
    "                                     ndim))\n",
    "\n",
    "for i in range(max_prior_length):\n",
    "    if DRAW_SAMPLES:\n",
    "        if __name__ == \"__main__\":\n",
    "            with Pool() as pool:\n",
    "                # uniform time prior of varying length\n",
    "                prior_dict['early'] = np.concatenate([[0.], [1./(i+1)]*(i+1)])\n",
    "\n",
    "                sampler = emcee.EnsembleSampler(nwalker, ndim, systm.likelihood, \n",
    "                                                args=[['early'], prior_dict], \n",
    "                                                kwargs={\"mode\": \"HMM\"},\n",
    "                                                moves=moves, pool=pool)\n",
    "                sampler.run_mcmc(theta0, nstep, progress=True)\n",
    "\n",
    "                # store the i-th sampling round\n",
    "                samples_multiT_HMM[i] = sampler.get_chain(flat=True, \n",
    "                                                          discard=burnin)\n",
    "\n",
    "        # saving the sampled data to disk for later convenience\n",
    "        np.save(f\"./samples/multiT_HMM_{i}.npy\", samples_multiT_HMM[i])\n",
    "        \n",
    "    else:\n",
    "        # loading in case we don't want to draw all the samples again\n",
    "        samples_multiT_HMM[i] = np.load(f\"./samples/multiT_HMM_{i}.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results vs the theoretical values. This is only applicable to the base probability rates $\\tilde{b}_1$ and $\\tilde{b}_2$, since all other LNL's probability rates are also influenced by other efferent spread pathways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean and variance of sampled parameters\n",
    "multiT_b = [np.mean(samples_multiT_HMM[:,:,i], axis=1) for i in range(4)]\n",
    "multiT_b_var = [np.var(samples_multiT_HMM[:,:,i], axis=1) for i in range(4)]\n",
    "multiT_t = [np.mean(samples_multiT_HMM[1:,:,i], axis=1) for i in range(4,7)]\n",
    "multiT_t_var = [np.var(samples_multiT_HMM[1:,:,i], axis=1) for i in range(4,7)]\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1,2, figsize=set_size(\"full\", ratio=2.8))\n",
    "\n",
    "# plot base prob sample means for differently lengthed time priors\n",
    "for i in range(4):\n",
    "    ax[0].plot(range(1,max_prior_length+1), multiT_b[i], \"o\", mfc=\"none\", ms=4, \n",
    "               label=f\"$\\\\tilde{{b}}_{{{i+1}}}$ sampled\");\n",
    "\n",
    "# prevalence of involvement in LNL I and II\n",
    "prevalence_I = np.sum(data[(\"path\", \"I\")].to_numpy()) / len(data)\n",
    "prevalence_II = np.sum(data[(\"path\", \"II\")].to_numpy()) / len(data)\n",
    "theory_roots = np.zeros(shape=(2,max_prior_length))\n",
    "\n",
    "# compute roots with P^* = prevalence\n",
    "for t in range(1,max_prior_length+1):\n",
    "    theory_roots[0,t-1] = sp.optimize.root_scalar(f, \n",
    "                                                  args=(t, prevalence_I), \n",
    "                                                  bracket=[0.001, 0.999]).root\n",
    "    theory_roots[1,t-1] = sp.optimize.root_scalar(f, \n",
    "                                                  args=(t, prevalence_II), \n",
    "                                                  bracket=[0.001, 0.999]).root\n",
    "\n",
    "# plot theoretical roots\n",
    "ax[0].plot(range(1,max_prior_length+1), theory_roots[0], \"-\", alpha=0.5, \n",
    "           label=r\"$\\tilde{b}_{1}$ theory\");\n",
    "ax[0].plot(range(1,max_prior_length+1), theory_roots[1], \"-\", alpha=0.5, \n",
    "           label=r\"$\\tilde{b}_{2}$ theory\");\n",
    "\n",
    "offset = 0.5\n",
    "ax[0].set_xlim([1 - offset, max_prior_length + offset]);\n",
    "ax[0].set_xticks(np.arange(1, max_prior_length+1, 2))\n",
    "ax[0].set_xlabel(\"length of prior $T$\");\n",
    "ax[0].set_ylabel(r\"Base Probability Rate $\\tilde{b}$\");\n",
    "ax[0].legend(ncol=1);\n",
    "\n",
    "\n",
    "# plot trans prob sample means\n",
    "# plot sample means for differently lengthed time priors\n",
    "for i in range(3):\n",
    "    ax[1].plot(range(2,max_prior_length+1), multiT_t[i], \"o\", mfc=\"none\", ms=4, \n",
    "               label=f\"$\\\\tilde{{t}}_{{{i+1}}}$ sampled\");\n",
    "    \n",
    "offset = 0.5\n",
    "ax[1].set_xlim([1 - offset, max_prior_length + offset]);\n",
    "ax[1].set_xticks(np.arange(1, max_prior_length+1, 2))\n",
    "ax[1].set_xlabel(\"length of prior $T$\");\n",
    "ax[1].set_ylabel(r\"Transition Probability Rate $\\tilde{t}$\");\n",
    "ax[1].legend(ncol=1);\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/rate_decay_theory_vs_sampled.png\", dpi=300, \n",
    "                bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/rate_decay_theory_vs_sampled.svg\", \n",
    "                bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots show that the probability rates of a more complex and realistic system qualitatively follow the same behaviour as the example with only one node. This serves as an argument why we can essentially choose the length of the prior as it suits us.\n",
    "\n",
    "Finally, let's check if the risk actually changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = {'T'  : ['I', 'II', 'III', 'IV'], \n",
    "         'I'  : ['II'], \n",
    "         'II' : ['III'], \n",
    "         'III': ['IV'], \n",
    "         'IV' : []}\n",
    "\n",
    "systm = lymph.System(graph=graph)\n",
    "\n",
    "# set specificity & sensitivity of diagnostic modality (here CT) manually\n",
    "spsn_dict = {\"CT\": [0.76, 0.81]}\n",
    "systm.set_modalities(spsn_dict=spsn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do we want to know, what do we know?\n",
    "inv = np.array([None, None, 1, None])  # we're interested in the risk of lvl 3 being involved\n",
    "# our observation is that lvl 2 is involved\n",
    "obs = {\"CT\": np.array([0, 1, 0, 0])}\n",
    "\n",
    "np.random.seed(SEED)\n",
    "ndim, nwalker, nstep, burnin = 7, 200, 2000, 1000\n",
    "thin = 200\n",
    "time_prior_dict = {}\n",
    "hmm_risk = np.zeros(shape=(max_prior_length, (nstep-burnin)*nwalker//thin))\n",
    "\n",
    "for k in range(max_prior_length):\n",
    "    # time priors\n",
    "    time_prior_dict['early'] = np.concatenate([[0.], [1./(k+1)] * (k+1)])\n",
    "\n",
    "    # risk for HMM and two different \"T-stages\" (early and late)\n",
    "    subset = np.random.permutation(samples_multiT_HMM[k])[::thin]\n",
    "    for i, sample in enumerate(subset):\n",
    "        systm.set_theta(sample)\n",
    "        hmm_risk[k, i] = systm.risk(inv=inv, obs=obs, \n",
    "                                    time_prior=time_prior_dict[\"early\"], \n",
    "                                    mode=\"HMM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_risk = np.zeros(shape=(len(samples_BN) // thin))\n",
    "for i, sample in enumerate(np.random.permutation(samples_BN)[::thin]):\n",
    "    systm.set_theta(sample)\n",
    "    bn_risk[i] = systm.risk(inv=inv, obs=obs, mode=\"BN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=set_size())\n",
    "\n",
    "ax.hist(100*hmm_risk[0], bins=100, range=[0., 50], density=True, \n",
    "        color=usz_red, alpha=0.1, label=f\"HMM ({max_prior_length} different $T$)\")\n",
    "[ax.hist(100*hmm_risk[k], bins=100, range=[0., 50], density=True, \n",
    "         color=usz_red, alpha=0.1) for k in np.arange(1,max_prior_length)];\n",
    "ax.hist(100*bn_risk, histtype=\"step\", bins=100, linewidth=2, range=[0., 50], \n",
    "        density=True, label=\"BN\");\n",
    "ax.set_xlim([0., 20]);\n",
    "ax.legend();\n",
    "ax.set_xlabel(r\"risk $R$ [%]\");\n",
    "ax.set_ylabel(\"PDF of risk\");\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/multi_length_risk.png\", dpi=300, \n",
    "                bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/multi_length_risk.svg\", \n",
    "                bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The risk for all the HMMs is the same, except for the one that only includes one time step. The reason for this is that when the system is not given the time to spread, it cannot correctly estimate the conditional risks of what would happen if the previous node was involved vs if it was not involved. So the HMM that is \"too short\" overestimates the risk due to the high prevalence of LNL III's involvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "235px",
    "width": "277px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "332.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "178d24c204a30672ea1fdde86877e76d36aba87ee79c03bdea66a58d070c8fdb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
